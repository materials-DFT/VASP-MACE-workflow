run: [train, val]

cutoff_radius: 5.0
chemical_symbols: [K, Mn,O]
model_type_names: ${chemical_symbols}
monitored_metric: val0_epoch/weighted_sum

data:
  _target_: nequip.data.datamodule.NequIPDataModule
  seed: 6699

  # --- split train.extxyz into train/val ---
  split_dataset:
    dataset:
      _target_: nequip.data.dataset.ASEDataset
      file_path: data/Nikhils_new_training_data_MnO2.extxyz


      include_keys: [REF_energy, REF_forces]

      key_mapping:
        REF_energy: total_energy
        REF_forces: forces

      transforms:
        - _target_: nequip.data.transforms.NeighborListTransform
          r_max: ${cutoff_radius}
        - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
          chemical_symbols: ${chemical_symbols}
    train: 0.95
    val: 0.05


  # --- dataloaders ---
  train_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 1
    shuffle: true
  val_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 1
  test_dataloader: ${data.val_dataloader}
  stats_manager:
    _target_: nequip.data.CommonDataStatisticsManager
    type_names: ${model_type_names}


trainer:
  _target_: lightning.Trainer
  max_epochs: 500
  check_val_every_n_epoch: 1
  log_every_n_steps: 1000

  accelerator: gpu
  devices: 1

  callbacks:
    - _target_: nequip.train.callbacks.SoftAdapt
      beta: 1.0
      interval: epoch
      frequency: 2

    # stop training when some criterion is met
    # - _target_: lightning.pytorch.callbacks.EarlyStopping
    #  monitor: ${monitored_metric}            # validation metric to monitor
    #  min_delta: 1e-4                         # how much to be considered a "change"
    #  patience: 20                            # how many instances of "no change" before stopping

    # checkpoint based on some criterion
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      monitor: ${monitored_metric}            # validation metric to monitor
      dirpath: models    # use hydra output directory
      filename: best                          # best.ckpt is the checkpoint name
      save_last: true                         # last.ckpt will be saved

    # log learning rate, e.g. to monitor what the learning rate scheduler is doing
    - _target_: lightning.pytorch.callbacks.LearningRateMonitor
      logging_interval: epoch

  logger:
    # Lightning wandb logger https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.wandb.html#module-lightning.pytorch.loggers.wandb
    _target_: lightning.pytorch.loggers.CSVLogger
    name: cohs
    save_dir: logs  # use resolver to place wandb logs in hydra's output directory


# NOTE:
# interpolation parameters for Allegro model
num_scalar_features: 64

training_module:
  _target_: nequip.train.EMALightningModule
  loss:
    _target_: nequip.train.EnergyForceLoss
    per_atom_energy: true
    coeffs:
      total_energy: 1.0
      forces: 10.0
  val_metrics:
    _target_: nequip.train.EnergyForceMetrics
    coeffs:
      per_atom_energy_mae: 1.0
      forces_mae: 1.0
  test_metrics: ${training_module.val_metrics}
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.001

  # ^ IMPORTANT: Allegro models do better with learning rates around 1e-3

  # to use the Allegro model in the NequIP framework, the following `model` block has to be changed to be that of Allegro's
  # see Allegro model docs for explanation of model hyperparameters:
  # https://nequip.readthedocs.io/projects/allegro/en/latest/guide/allegro_model.html
  model:
    _target_: allegro.model.AllegroModel

    # If you have PyTorch >= 2.6.0 installed, and are training on GPUs, the following line uses torch.compile to speed up training
    # for more details, see https://nequip.readthedocs.io/en/latest/guide/accelerations/pt2_compilation.html
    # compile_mode: compile
    # ^ if you're using PyTorch <= 2.6.0, an error will be thrown -- comment out the line to avoid it

    # === basic model params ===
    seed: 456
    model_dtype: float64
    type_names: ${model_type_names}
    r_max: ${cutoff_radius}

    # === two-body scalar embedding ===
    radial_chemical_embed:
      # the defaults for the Bessel embedding module are usually appropriate
      _target_: allegro.nn.TwoBodyBesselScalarEmbed
      num_bessels: 8
      bessel_trainable: false
      polynomial_cutoff_p: 6

    # output dimension of the radial-chemical embedding
    radial_chemical_embed_dim: ${num_scalar_features}

    # scalar embedding MLP
    scalar_embed_mlp_hidden_layers_depth: 1
    scalar_embed_mlp_hidden_layers_width: ${num_scalar_features}
    scalar_embed_mlp_nonlinearity: silu

    # === core hyperparameters ===
    # The following hyperparameters are the main ones that one should focus on tuning.

    # maximum order l to use in spherical harmonics embedding, 1 is baseline (fast), 2 is more accurate, but slower, 3 highly accurate but slow
    l_max: 2

    # number of tensor product layers, 1-3 usually best, more is more accurate but slower
    num_layers: 3

    # number of scalar features, more is more accurate but slower
    # 16, 32, 64, 128, 256 are good options to try depending on the dataset
    num_scalar_features: ${num_scalar_features}

    # number of tensor features, more is more accurate but slower
    # 8, 16, 32, 64 are good options to try depending on the dataset
    num_tensor_features: 64

    # == allegro MLPs ==
    # neural network parameters in the Allegro layers
    allegro_mlp_hidden_layers_depth: 1
    allegro_mlp_hidden_layers_width: ${num_scalar_features}
    allegro_mlp_nonlinearity: silu
    # ^ setting `nonlinearity` to `null` means that the Allegro MLPs are effectively linear layers

    # === advanced hyperparameters ===
    # The following hyperparameters should remain in their default states until the above core hyperparameters have been set.

    # whether to include features with odd mirror parity
    # often turning parity off gives equally good results but faster networks, so do consider this
    parity: false

    # whether the tensor product weights couple the paths and channels or not (otherwise the weights are only applied per-path)
    # default is `true`, which is expected to be more expressive than `false`
    tp_path_channel_coupling: true

    # == readout MLP ==
    # neural network parameters in the readout layer
    readout_mlp_hidden_layers_depth: 1
    readout_mlp_hidden_layers_width: ${num_scalar_features}
    readout_mlp_nonlinearity: silu
    # ^ setting `nonlinearity` to `null` means that output MLP is effectively a linear layer

    # === misc hyperparameters ===
    # average number of neighbors for edge sum normalization
    avg_num_neighbors: ${training_data_stats:num_neighbors_mean}

    # # per-type per-atom scales and shifts
    # per_type_energy_shifts:
    #   Li: -0.775748670 # Found here /usr/workspace/ml-devel/simulations/Li_atomic_energy
    #   F: -2.246679560 # Found here /usr/workspace/ml-devel/simulations/F_atomic_energy
      
    per_type_energy_scales: ${training_data_stats:forces_rms}
    per_type_energy_scales_trainable: true
    per_type_energy_shifts_trainable: false

global_options:
  allow_tf32: true
